---
---



{% include header.html %}


<h2>
Services
</h2>

<p>
We hear a new world of sound that is ever present, ever changing and one that we own, since it originates within us.</p>

<p>
We are creating the sound of the 2020's today - collaborative, contextual, behaviouristic biomusic: informed by nature, made possible though science, we create possibilities for anyone with a pulse to enjoy an emergent, authentic and personally satisfying experience that goes beyond music and genres, straight to the body and directly to the brain.</p>

<p>
We would like you to join us in our musical quest to unite people, places and things in a new, infinite world of sound.</p>

<p> 
Sign up for the beta <a href="mailto:ove@biomusic.cc">here</a>

<h4>How it works</h4>

<p>We want to make a continuous, dynamic music experience out of everyday life, whatever it may be. The world and our actions in it form a complex, adaptive event process, which we turn into sounds that follow musical rules and interact with each other.</p>

<p>This kind of music is relevant, informative and engaging, as it requires no active operation, but uses natural movements and inferred states to reflect users’ relationship with the surrounding environment.</p>

<p>The environment provides musical matter, energy and information, including that of other users. We are developing a massively multi-performer music system based on social music and emotional contagion that takes holarchic form.</p>

<p>Sensor data from the phone is processed and mapped to musical parameters. Information pulled from web services or other devices in your surroundings contribute to the synthesis model.</p>

<p>Biomechanical movements and other activities create a dynamic soundscape to be physically explored in time. 
Optional wearables, like activity bands or sports watches, measure heart rate, skin temperature, galvanic skin response, movement and more.</p>

<p>Respiration phase, amplitude and rate creates a natural breath controller and provides a strong biofeedback channel. Heart rates between users can be synchronised (we support Ableton Link). we can already send and receive musical phrases that depend on body postures, gestures, distance or context. The app learns and detects gestures and hand positions. You can also set locations and beacons for further interactions.</p>

<p>When in a vehicle, the tempo automatically adapts to the speed of the vehicle as it enters a special car mode. Altitude, weather and position of the vehicle forms part of the music. Course, heading and proximity to other elements create higher level changes in the music. A future version lets nearby districts and buildings influence the music, as determined by land use and other factors specified by the map provider. Waypoints, destinations and landmarks influence musical parameters from a distance. Their proximity, character and bearing is made audible in the music.</p>

<p>In addition to already using geofences, we set up Estimote beacons in special locations, like shops. Footfall and dwell time become musical parameters. B2B customers can announce and define their physical presence musically. Coupling places and movement with musical entrainment redefines our relationship to music and results in the embodiment of sound.</p>

<h4>Partners</h4>

<p>Our hardware ecosystem now includes Suunto’s Movesense platform and soon the upcoming respiration sensors from our partners iBreve. We welcome other modules, sensors and APIs to be supported!</p>

<h4>B2B services</h4>

<p>You can include our tech in your app or your event. We are happy to provide:</p>
<p>-custom development based on our tech</p>
<p>-licensed or white label apps</p>
  
<h4>Case studies</h4>

<p>
<b>Beddit</b> tracks your sleep using a thin film cardioballistography sensor on the mattress. We prototyped and created different smart alarm versions and sleep musification models. The product was adapted to be used for sonification and visualisation of biosignals at marketing events. 
</p>

<p><img src="static/img/beddit_slush.jpg", width="460px"/>
<small><i>Beddit demo, Slush 2014.</i></small></p>

<p><b>Marfle</b>: We researched and created a vehicular sonification for ships, using technology by Marfle, a fleet monitoring company. A limited edition vinyl was produced from the resulting music.
                                                                                                    

<p><img src="static/img/helsinki_design_week.jpeg", width="460px"/>
<small><i>Helsinki Design Week 2015.</i></small></p>


<h4>Science</h4>

<p>
Producing expressive and entertaining music based on biosignal measurement is
an interesting research topic. We have so far published one paper on our technology, at the
<a href="http://ida2012.org/">11th International Symposium on Intelligent Data Analysis</a>
in October 2012.
</p>

<p><small>Paalasmaa, J., Murphy, D. J. &amp; Holmqvist, O., 2012.
<b>Analysis of Noisy Biosignals for Musical Performance.</b>
<span style="font-style:italic;">11th International Symposium on Intelligent Data Analysis (IDA 2012)</span>. Lecture Notes in Computer Science 7619. pp. 241-252. Springer, Heidelberg.
<a href="https://helda.helsinki.fi/bitstream/handle/10138/37400/paalasmaa_2012_analysis.pdf">download PDF</a> &#8211;
<a href="http://dx.doi.org/10.1007/978-3-642-34156-4_23">doi: 10.1007/978-3-642-34156-4_23</a></small></p>


{% include footer.html %}
